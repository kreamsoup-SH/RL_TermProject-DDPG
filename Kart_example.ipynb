{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb2537f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfigurationChannel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e78390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 12*4\n",
    "action_size = 2\n",
    "\n",
    "load_model = False\n",
    "train_mode = True\n",
    "\n",
    "batch_size = 128\n",
    "mem_maxlen = 10000\n",
    "discount_factor = 0.9\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 5e-4\n",
    "tau = 1e-3\n",
    "\n",
    "mu = 0\n",
    "theta = 1e-3\n",
    "sigma = 2e-3\n",
    "\n",
    "train_step = 300000 if train_mode else 0\n",
    "save_step = 10000\n",
    "test_step = 10000\n",
    "train_start_step = 2000\n",
    "\n",
    "print_interval = 10\n",
    "save_interval = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = \"Kart1.exe\"\n",
    "file_path = f\"./{game}\"\n",
    "save_path = f\"./\"\n",
    "load_path = f\"./\"\n",
    "\n",
    "engine_configuration_channel = EngineConfigurationChannel()\n",
    "env = UnityEnvironment(file_name=file_path,\n",
    "                       side_channels=[engine_configuration_channel],\n",
    "                       worker_id=1)\n",
    "env.reset()\n",
    "behavior_name = list(env.behavior_specs.keys())[0]\n",
    "spec = env.behavior_specs[behavior_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.54264129, -0.9584961 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(seed=10)\n",
    "np.array([np.random.rand(2)*2-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26729647, 1.        ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array([[np.random.rand()*2-1,1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Actor Network\n",
    "    Input: state\n",
    "    Output: action\n",
    "    Update: actor_optimizer\n",
    "Critic Network\n",
    "    Input: state, action\n",
    "    Output: Q-value\n",
    "    Update: critic_optimizer    \n",
    "'''\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        action = F.softmax(self.fc3(x))\n",
    "        return action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.gelu(self.fc1(torch.cat([state, action], dim=1)))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value\n",
    "\n",
    "class OrnsteinUhlenbeckNoise:\n",
    "    def __init__(self, action_dim, mu=0, theta=0.15, sigma=0.2):\n",
    "        self.action_dim = action_dim\n",
    "        self.mu = mu\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(len(x))\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "        \n",
    "class DDPG_1:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99):\n",
    "        self.actor = Actor(state_dim, action_dim)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.noise = OrnsteinUhlenbeckNoise(action_dim)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state).squeeze().numpy()\n",
    "        noise = self.noise.sample()\n",
    "        return np.clip(action + noise, -1, 1)  # assuming action space is -1 to 1\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.FloatTensor(action)\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        done = torch.FloatTensor([done])\n",
    "\n",
    "        # Compute the Q-value target\n",
    "        target_value = reward + (1.0 - done) * self.gamma * self.critic(next_state, self.actor(next_state))\n",
    "\n",
    "        # Update the critic\n",
    "        current_value = self.critic(state, action)\n",
    "        critic_loss = nn.MSELoss()(current_value, target_value)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update the actor\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN - YT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "buffer_limit  = 50000\n",
    "batch_size    = 32\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "    \n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "    \n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        state_list, action_list, reward_list, s_prime_list, done_mask_list = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            state_list.append(s)\n",
    "            action_list.append([a])\n",
    "            reward_list.append([r])\n",
    "            s_prime_list.append(s_prime)\n",
    "            done_mask_list.append([done_mask])\n",
    "\n",
    "        return torch.tensor(state_list, dtype=torch.float), torch.tensor(action_list), \\\n",
    "               torch.tensor(reward_list), torch.tensor(s_prime_list, dtype=torch.float), \\\n",
    "               torch.tensor(done_mask_list)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, STATE_DIM, ACTION_DIM):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(STATE_DIM, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, ACTION_DIM)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.gelu(self.fc1(x))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        action = F.softmax(self.fc3(x))\n",
    "        return action\n",
    "    \n",
    "    def sample_action(self, state, epsilon):\n",
    "        if epsilon > random.random():\n",
    "            return np.array([[np.random.rand()*2-1,1]])\n",
    "            \n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.forward(state).sqeeze().numpy()\n",
    "        return action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, STATE_DIM, ACTION_DIM):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(STATE_DIM + ACTION_DIM, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = F.gelu(self.fc1(torch.cat([state, action], dim=1)))\n",
    "        x = F.gelu(self.fc2(x))\n",
    "        value = self.fc3(x)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG():\n",
    "    def __init__(self, STATE_DIM, ACTION_DIM, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99):\n",
    "        self.actor = Actor(STATE_DIM, ACTION_DIM)\n",
    "        self.critic = Critic(STATE_DIM, ACTION_DIM)\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state = torch.FloatTensor(state)\n",
    "        action = torch.FloatTensor(action)\n",
    "        reward = torch.FloatTensor([reward])\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        done = torch.FloatTensor([done])\n",
    "\n",
    "        # Compute the Q-value target\n",
    "        target_value = reward + (1.0 - done) * self.gamma * self.critic(next_state, self.actor(next_state))\n",
    "\n",
    "        # Update the critic\n",
    "        current_value = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(current_value, target_value)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Update the actor\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10):\n",
    "        s,a,r,s_prime,done_mask = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q(s)\n",
    "        q_a = q_out.gather(1,a)\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DDPG(STATE_DIM, ACTION_DIM,lr_actor=actor_lr, lr_critic=critic_lr, gamma=0.99)\n",
    "model.actor.load_state_dict(q.state_dict())\n",
    "memory = ReplayBuffer()\n",
    "\n",
    "print_interval = 20\n",
    "score = 0.0  \n",
    "optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "for n_epi in range(10000):\n",
    "    epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) #Linear annealing from 8% to 1%\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        a = q.sample_action(torch.from_numpy(s).float(), epsilon)      \n",
    "        s_prime, r, done, truncated, info = env.step(a)\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "        memory.put((s,a,r/100.0,s_prime, done_mask))\n",
    "        s = s_prime\n",
    "\n",
    "        score += r\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if memory.size()>2000:\n",
    "        train(q, q_target, memory, optimizer)\n",
    "\n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        q_target.load_state_dict(q.state_dict())\n",
    "        print(\"n_episode :{}, score : {:.1f}, n_buffer : {}, eps : {:.1f}%\".format(\n",
    "                                                        n_epi, score/print_interval, memory.size(), epsilon*100))\n",
    "        score = 0.0\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO - ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma         = 0.98\n",
    "lmbda         = 0.95\n",
    "eps_clip      = 0.1\n",
    "K_epoch       = 3\n",
    "T_horizon     = 20\n",
    "\n",
    "class PPO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PPO, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1   = nn.Linear(4,256)\n",
    "        self.fc_pi = nn.Linear(256,2)\n",
    "        self.fc_v  = nn.Linear(256,1)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def pi(self, x, softmax_dim = 0):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "      \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self):\n",
    "        state_list, action_list, reward_list, s_prime_list, prob_action_list, done_list = [], [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            state, action, reward, s_prime, prob_a, done = transition\n",
    "            \n",
    "            state_list.append(state)\n",
    "            action_list.append([action])\n",
    "            reward_list.append([reward])\n",
    "            s_prime_list.append(s_prime)\n",
    "            prob_action_list.append([prob_a])\n",
    "            done_mask = 0 if done else 1\n",
    "            done_list.append([done_mask])\n",
    "            \n",
    "        s,a,r,s_prime,done_mask, prob_a =   torch.tensor(state_list, dtype=torch.float), \\\n",
    "                                            torch.tensor(action_list), \\\n",
    "                                            torch.tensor(reward_list), \\\n",
    "                                            torch.tensor(s_prime_list, dtype=torch.float), \\\n",
    "                                            torch.tensor(done_list, dtype=torch.float), \\\n",
    "                                            torch.tensor(prob_action_list)\n",
    "        self.data = []\n",
    "        return s, a, r, s_prime, done_mask, prob_a\n",
    "        \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done_mask, prob_a = self.make_batch()\n",
    "\n",
    "        for i in range(K_epoch):\n",
    "            td_target = r + gamma * self.v(s_prime) * done_mask\n",
    "            delta = td_target - self.v(s)\n",
    "            delta = delta.detach().numpy()\n",
    "\n",
    "            advantage_lst = []\n",
    "            advantage = 0.0\n",
    "            for delta_t in delta[::-1]:\n",
    "                advantage = gamma * lmbda * advantage + delta_t[0]\n",
    "                advantage_lst.append([advantage])\n",
    "            advantage_lst.reverse()\n",
    "            advantage = torch.tensor(advantage_lst, dtype=torch.float)\n",
    "\n",
    "            pi = self.pi(s, softmax_dim=1)\n",
    "            pi_a = pi.gather(1,a)\n",
    "            ratio = torch.exp(torch.log(pi_a) - torch.log(prob_a))  # a/b == exp(log(a)-log(b))\n",
    "\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
    "            loss = -torch.min(surr1, surr2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main -for me\n",
    "env = gym.make('CartPole-v1')\n",
    "model = PPO()\n",
    "score = 0.0\n",
    "print_interval = 20\n",
    "for n_epi in range(10000):\n",
    "    state = 받아와야함\n",
    "    done = False\n",
    "    while not done:\n",
    "        for t in range(T_horizon):\n",
    "            prob = model.pi(torch.from_numpy(state).float())\n",
    "            m = Categorical(prob)\n",
    "            a = m.sample().item()\n",
    "            s_prime, r, done, truncated, info = env.step(state)\n",
    "\n",
    "            model.put_data((state, a, r/100.0, s_prime, prob[a].item(), done))\n",
    "            state = s_prime\n",
    "\n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        model.train_net()\n",
    "\n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "        score = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "env = gym.make('CartPole-v1')\n",
    "model = PPO()\n",
    "score = 0.0\n",
    "print_interval = 20\n",
    "for n_epi in range(10000):\n",
    "    s, _ = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        for t in range(T_horizon):\n",
    "            prob = model.pi(torch.from_numpy(s).float())\n",
    "            m = Categorical(prob)\n",
    "            a = m.sample().item()\n",
    "            s_prime, r, done, truncated, info = env.step(a)\n",
    "\n",
    "            model.put_data((s, a, r/100.0, s_prime, prob[a].item(), done))\n",
    "            s = s_prime\n",
    "\n",
    "            score += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        model.train_net()\n",
    "\n",
    "    if n_epi%print_interval==0 and n_epi!=0:\n",
    "        print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "        score = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(48, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "\n",
    "# Define the value network\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(48, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.fc(state)\n",
    "\n",
    "# Define the PPO algorithm\n",
    "class PPO:\n",
    "    def __init__(self, policy, value, policy_optimizer, value_optimizer, clip_epsilon=0.2, epochs=10):\n",
    "        self.policy = policy\n",
    "        self.value = value\n",
    "        self.policy_optimizer = policy_optimizer\n",
    "        self.value_optimizer = value_optimizer\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def update(self, old_log_probs, states, actions, returns, advantages):\n",
    "        for _ in range(self.epochs):\n",
    "            # Policy update\n",
    "            mu = self.policy(states)\n",
    "            std = torch.tensor([0.1, 0.1])  # You might want to learn this as well\n",
    "            dist = Normal(mu, std)\n",
    "            log_prob = dist.log_prob(actions)\n",
    "            ratio = (log_prob - old_log_probs).exp()\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1.0 - self.clip_epsilon, 1.0 + self.clip_epsilon) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            self.policy_optimizer.step()\n",
    "\n",
    "            # Value update\n",
    "            value_loss = ((returns - self.value(states)) ** 2).mean()\n",
    "\n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()\n",
    "\n",
    "policy = PolicyNetwork()\n",
    "value = ValueNetwork()\n",
    "policy_optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "value_optimizer = optim.Adam(value.parameters(), lr=1e-3)\n",
    "ppo = PPO(policy, value, policy_optimizer, value_optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_epi' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\ssdworkspace\\RLcart\\RL_TermProject\\Kart_example.ipynb Cell 23\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ssdworkspace/RLcart/RL_TermProject/Kart_example.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m episode_list \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ssdworkspace/RLcart/RL_TermProject/Kart_example.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m ep \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episode_num):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/ssdworkspace/RLcart/RL_TermProject/Kart_example.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     epsilon \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39m1e-2\u001b[39m, (\u001b[39m8e-2\u001b[39m)\u001b[39m-\u001b[39m(\u001b[39m1e-2\u001b[39m)\u001b[39m*\u001b[39m(n_epi\u001b[39m/\u001b[39m\u001b[39m200\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ssdworkspace/RLcart/RL_TermProject/Kart_example.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/ssdworkspace/RLcart/RL_TermProject/Kart_example.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     decision_steps, terminal_steps \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mget_steps(behavior_name)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'n_epi' is not defined"
     ]
    }
   ],
   "source": [
    "episode_num = 3\n",
    "episode_list = []\n",
    "\n",
    "for ep in range(episode_num):\n",
    "    epsilon = max(1e-2, (8e-2)-(1e-2)*(n_epi/200))\n",
    "    env.reset()\n",
    "    decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "    tracked_agent = -1\n",
    "    done = False\n",
    "    ep_rewards = 0\n",
    "    \n",
    "    # init action probability\n",
    "    # action_prob = np.array([np.random.rand(2)*2-1])\n",
    "\n",
    "    # init episode record\n",
    "    action_list = []\n",
    "    state_list = []\n",
    "\n",
    "    while not done:\n",
    "        if tracked_agent == -1 and len(decision_steps) >= 1:\n",
    "            tracked_agent = decision_steps.agent_id[0]\n",
    "        \n",
    "        \"\"\"action\"\"\"\n",
    "        # action = np.array([[np.random.rand()*2-1,1]])\n",
    "        # action = sample_action(torch.from_numpy(s).float(), epsilon)\n",
    "\n",
    "        \n",
    "        action_tuple = ActionTuple()\n",
    "        print(f\"{ep=}\",end=' \\ ')\n",
    "        action_tuple.add_continuous(action)\n",
    "        print(f\"{action=}\")\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "        decision_steps, terminal_steps = env.get_steps(behavior_name)\n",
    "\n",
    "        ####\n",
    "        current_state = decision_steps.obs[0]\n",
    "        terminal_state = terminal_steps.obs[0]\n",
    "        print(f\"{current_state=}\\n{teminal_state=}\")\n",
    "\n",
    "        action_list.append(action)\n",
    "        state_list.append(current_state)\n",
    "        if terminal_state.size > 0:\n",
    "            episode_list.append([action_list,state_list])\n",
    "            done=True\n",
    "        \n",
    "        ####\n",
    "        ep_rewards += sum(terminal_steps) # 내가 원하는 합으로 수정해야함\n",
    "        print(f\"{ep_rewards=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.27888732,  1.        ]]),\n",
       " array([[0.07925139, 1.        ]]),\n",
       " array([[0.84834243, 1.        ]]),\n",
       " array([[0.13605731, 1.        ]]),\n",
       " array([[0.4503492, 1.       ]]),\n",
       " array([[0.64136846, 1.        ]]),\n",
       " array([[-0.95854065,  1.        ]]),\n",
       " array([[-0.87348785,  1.        ]]),\n",
       " array([[0.66416878, 1.        ]]),\n",
       " array([[0.02361138, 1.        ]]),\n",
       " array([[-0.49751712,  1.        ]]),\n",
       " array([[-0.6713075,  1.       ]]),\n",
       " array([[0.88616192, 1.        ]]),\n",
       " array([[0.33738406, 1.        ]]),\n",
       " array([[-0.91918758,  1.        ]]),\n",
       " array([[-0.3419065,  1.       ]]),\n",
       " array([[0.5567389, 1.       ]]),\n",
       " array([[-0.65967951,  1.        ]]),\n",
       " array([[0.27326759, 1.        ]]),\n",
       " array([[-0.27440299,  1.        ]]),\n",
       " array([[0.15318237, 1.        ]]),\n",
       " array([[0.03314612, 1.        ]]),\n",
       " array([[0.2753034, 1.       ]]),\n",
       " array([[-0.45096082,  1.        ]]),\n",
       " array([[0.41525523, 1.        ]]),\n",
       " array([[-0.31716341,  1.        ]]),\n",
       " array([[-0.84728946,  1.        ]]),\n",
       " array([[-0.60685096,  1.        ]]),\n",
       " array([[-0.25424936,  1.        ]]),\n",
       " array([[0.39182452, 1.        ]]),\n",
       " array([[0.29432064, 1.        ]]),\n",
       " array([[-0.56854471,  1.        ]]),\n",
       " array([[0.45124013, 1.        ]]),\n",
       " array([[-0.29704687,  1.        ]]),\n",
       " array([[0.21751604, 1.        ]]),\n",
       " array([[-0.51455758,  1.        ]]),\n",
       " array([[0.10696611, 1.        ]]),\n",
       " array([[-0.67803437,  1.        ]]),\n",
       " array([[0.24428003, 1.        ]]),\n",
       " array([[0.26132513, 1.        ]]),\n",
       " array([[0.31143863, 1.        ]]),\n",
       " array([[-0.6290489,  1.       ]]),\n",
       " array([[-0.58677148,  1.        ]]),\n",
       " array([[0.37533629, 1.        ]]),\n",
       " array([[-0.82354133,  1.        ]]),\n",
       " array([[-0.31904897,  1.        ]]),\n",
       " array([[0.04301069, 1.        ]]),\n",
       " array([[-0.07292895,  1.        ]]),\n",
       " array([[0.05621036, 1.        ]]),\n",
       " array([[-0.61075562,  1.        ]]),\n",
       " array([[-0.70021131,  1.        ]]),\n",
       " array([[0.57853965, 1.        ]]),\n",
       " array([[0.12932391, 1.        ]]),\n",
       " array([[0.39461767, 1.        ]]),\n",
       " array([[-0.76375829,  1.        ]]),\n",
       " array([[0.98482054, 1.        ]]),\n",
       " array([[-0.11317492,  1.        ]]),\n",
       " array([[0.24074914, 1.        ]]),\n",
       " array([[0.13060534, 1.        ]]),\n",
       " array([[-0.95931951,  1.        ]]),\n",
       " array([[-0.40999573,  1.        ]]),\n",
       " array([[0.02241678, 1.        ]]),\n",
       " array([[-0.81619469,  1.        ]]),\n",
       " array([[-0.92135372,  1.        ]]),\n",
       " array([[0.69590818, 1.        ]]),\n",
       " array([[0.02852453, 1.        ]]),\n",
       " array([[0.75246472, 1.        ]]),\n",
       " array([[0.66700369, 1.        ]]),\n",
       " array([[0.58624749, 1.        ]]),\n",
       " array([[0.66522275, 1.        ]]),\n",
       " array([[-0.6441768,  1.       ]]),\n",
       " array([[-0.61455577,  1.        ]]),\n",
       " array([[0.06160585, 1.        ]]),\n",
       " array([[-0.28767196,  1.        ]]),\n",
       " array([[-0.83072307,  1.        ]]),\n",
       " array([[0.70612582, 1.        ]]),\n",
       " array([[-0.21155092,  1.        ]]),\n",
       " array([[0.28257242, 1.        ]]),\n",
       " array([[-0.56918308,  1.        ]]),\n",
       " array([[0.55202541, 1.        ]]),\n",
       " array([[0.18872646, 1.        ]]),\n",
       " array([[-0.66546807,  1.        ]]),\n",
       " array([[-0.16245712,  1.        ]]),\n",
       " array([[-0.95201325,  1.        ]]),\n",
       " array([[-0.86470223,  1.        ]]),\n",
       " array([[-0.60140619,  1.        ]]),\n",
       " array([[0.55257808, 1.        ]]),\n",
       " array([[-0.52760423,  1.        ]]),\n",
       " array([[-0.84295224,  1.        ]]),\n",
       " array([[0.48903485, 1.        ]]),\n",
       " array([[0.78586927, 1.        ]]),\n",
       " array([[-0.86539916,  1.        ]]),\n",
       " array([[-0.05674137,  1.        ]]),\n",
       " array([[-0.7880844,  1.       ]]),\n",
       " array([[0.14105177, 1.        ]]),\n",
       " array([[-0.00234641,  1.        ]]),\n",
       " array([[-0.38688085,  1.        ]]),\n",
       " array([[0.44234712, 1.        ]]),\n",
       " array([[-0.63585092,  1.        ]]),\n",
       " array([[-0.5137546,  1.       ]]),\n",
       " array([[0.12803347, 1.        ]]),\n",
       " array([[0.54895819, 1.        ]]),\n",
       " array([[0.73174681, 1.        ]]),\n",
       " array([[0.0896215, 1.       ]]),\n",
       " array([[0.4760662, 1.       ]]),\n",
       " array([[-0.20894662,  1.        ]]),\n",
       " array([[0.04279184, 1.        ]]),\n",
       " array([[0.21740764, 1.        ]]),\n",
       " array([[0.45806318, 1.        ]]),\n",
       " array([[0.59624707, 1.        ]]),\n",
       " array([[0.17077413, 1.        ]]),\n",
       " array([[0.76756601, 1.        ]]),\n",
       " array([[-0.96664665,  1.        ]]),\n",
       " array([[-0.51230297,  1.        ]]),\n",
       " array([[0.47651015, 1.        ]]),\n",
       " array([[-0.52443967,  1.        ]]),\n",
       " array([[-0.05070303,  1.        ]]),\n",
       " array([[0.12308407, 1.        ]]),\n",
       " array([[-0.75454608,  1.        ]]),\n",
       " array([[-0.82872626,  1.        ]]),\n",
       " array([[0.07632489, 1.        ]]),\n",
       " array([[-0.79570407,  1.        ]]),\n",
       " array([[-0.39956616,  1.        ]]),\n",
       " array([[-0.39479851,  1.        ]]),\n",
       " array([[0.92124544, 1.        ]]),\n",
       " array([[0.51743293, 1.        ]]),\n",
       " array([[0.7650588, 1.       ]]),\n",
       " array([[-0.67517585,  1.        ]]),\n",
       " array([[-0.56120426,  1.        ]]),\n",
       " array([[-0.73634124,  1.        ]]),\n",
       " array([[0.63562885, 1.        ]]),\n",
       " array([[0.44906593, 1.        ]]),\n",
       " array([[-0.03139609,  1.        ]]),\n",
       " array([[-0.45035435,  1.        ]]),\n",
       " array([[-0.16429501,  1.        ]]),\n",
       " array([[0.43152478, 1.        ]]),\n",
       " array([[0.54600033, 1.        ]]),\n",
       " array([[0.11599018, 1.        ]]),\n",
       " array([[0.37504307, 1.        ]]),\n",
       " array([[-0.8756181,  1.       ]]),\n",
       " array([[-0.92664468,  1.        ]]),\n",
       " array([[-0.87924849,  1.        ]]),\n",
       " array([[0.2819276, 1.       ]]),\n",
       " array([[0.93486814, 1.        ]]),\n",
       " array([[0.58304831, 1.        ]]),\n",
       " array([[0.19904271, 1.        ]]),\n",
       " array([[0.24563703, 1.        ]]),\n",
       " array([[0.25691008, 1.        ]]),\n",
       " array([[0.62978137, 1.        ]]),\n",
       " array([[-0.93551838,  1.        ]]),\n",
       " array([[-0.80261475,  1.        ]]),\n",
       " array([[0.89838845, 1.        ]]),\n",
       " array([[-0.06996207,  1.        ]]),\n",
       " array([[-0.45732942,  1.        ]]),\n",
       " array([[-0.49905556,  1.        ]]),\n",
       " array([[0.00335431, 1.        ]]),\n",
       " array([[-0.3367507,  1.       ]]),\n",
       " array([[0.86140628, 1.        ]]),\n",
       " array([[0.85834417, 1.        ]]),\n",
       " array([[-0.09215769,  1.        ]]),\n",
       " array([[0.91014925, 1.        ]]),\n",
       " array([[0.95058297, 1.        ]]),\n",
       " array([[0.56049871, 1.        ]]),\n",
       " array([[-0.45748723,  1.        ]]),\n",
       " array([[0.1118477, 1.       ]]),\n",
       " array([[0.08379252, 1.        ]]),\n",
       " array([[-0.4760303,  1.       ]]),\n",
       " array([[-0.11531162,  1.        ]]),\n",
       " array([[0.97355034, 1.        ]]),\n",
       " array([[-0.26483348,  1.        ]]),\n",
       " array([[0.15513218, 1.        ]]),\n",
       " array([[0.3126081, 1.       ]]),\n",
       " array([[-0.00581018,  1.        ]]),\n",
       " array([[-0.66838759,  1.        ]]),\n",
       " array([[0.80177049, 1.        ]]),\n",
       " array([[0.17887057, 1.        ]])]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_list[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
